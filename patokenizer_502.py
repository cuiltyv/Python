# -*- coding: utf-8 -*-
"""patokenizer - 501.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ljNt_3R00hPkvjMDYXHHO2m7q3ISDjmJ
"""

import re

#! Leer el archivo

with open('program.txt', 'r') as file:
    lines = file.read()
    print(lines)  # This will be a list of lines


# Estes es el patron
# \w es una abreviacion de [a-zA-Z0-9_]
# \b es el fin del match
pattern = r"^[a-zA-Z]\w*\b"


# Ejemplo sin match
text2 = "12aa123"
match2 = re.match(pattern, text2)
# print(match2)


# Ejemplo de match
text1 = "abcd_123"
match1 = re.match(pattern, text1)
# print(match1)


# print(match1.group())

class Tokenizer:
  tokens = []

  re_table = {}       #   un diccionario de expresiones regulares
  keyword_list = []   #   una lista de keywords

  # El objeto token deberia ser inicializado con.... ?
  #     el diccionario de expresiones
  #     la lista de keywords
  def __init__(self, diccregex, kwlist) :
    self.re_table = diccregex
    self.keyword_list = kwlist
    self.tokens = []
    self.symbol_table = {}

  def best_match(self, input_string):
    matches = []
    for key in self.re_table:
      label = key #label del token
      regrex = self.re_table[label]

      matched = re.match(regrex, input_string) #Busca
      if matched:
        matches.append([label, matched.group()]) #Label del token y substring
    
    if len(matches) == 0:
      return False
    
    longest = max(matches, key = lambda x : len(x[1]))

    if longest[0] == "identifier":
      if longest[1] in self.keyword_list:
        longest[0] = "kw_" + longest[1]
    return longest

  def prints(self):

    print()
    print("Tabla de simbolos")

    for substring, attributes in self.symbol_table.items():
      for (token,line,index) in attributes:
        print(f"{token} -> {substring}   Linea -> {line}   Index -> {index}")

  # Recibe la cadena a tokenizar
  # Devuelve el stream de tokens
  def tokenize(self, input_string):

    split_line_input = input_string.splitlines()  
    
    for indx, line in enumerate(split_line_input):
      print()
      print(f"Line {indx}")
      counter = 0
      while True:
        line = line.strip()
        if line != "":
            best = self.best_match(line)
            if best != False:
              token = best[0]
              substring = best[1]
              if(token == "identifier"):
                if(substring in self.symbol_table): #Token, Line, Position
                  self.symbol_table[substring].append((token, indx, counter))
                else:
                  self.symbol_table[substring] = [(token, indx, counter)]
              print("Found best match", token, substring)
              counter += len(substring)
              self.tokens.append(best)
              line = line[len(substring):]
            else:
              print("No matches found")
              break
        else:
          if counter == 0:
            print('No hay tokens en esta linea')
          break
        
    print()
    return self.tokens
    # El ciclo del algoritmo:
      # remueve blanks al inicio de input_string
      # Si input no esta vacia:
      #   Busca el matching substring mas largo a partir del inicio de input_substring
      #   Si hay un match:
      #     Agrega el match a la lista
      #     Remueve el matching substring al inicio de input_string
      #   Si no: Error!!
      

dictio_regex = { 'const_int' : r"^[0-9]+\b",
				'identifier' : r"^[a-zA-Z_]\w*\b",
				'=' : r"^=",
        '==' : r"^==",
        ';' : r"^;",
        'operator' : r"^[-+*/]",
        ',' : r"^,",
        '(' : r"^\(",
        ')' : r"^\)",
        '[' : r"^\[",
        ']' : r"^\]",
        '{' : r"^\{",
        '}' : r"^\}",
        '>' : r"^>",
        '<' : r"^<",
        '!=' : r"^!=",
        '<=' : r"^<=",
        '>=' : r"^>=",
        'comment' : r"^#.*",
        ':' : r"^:",
        'const_float' : r"^[0-9]+\.[0-9]+",
        'string' : r"^\"[^\"]*\"", 
      }


keyword_list = ["print", "main", "program", "var", "int", "float", "do", "while", "if", "else", "void", "string"]


input_program = """  == a = 14;
  var_b = < 4;
  \{
  program(4)
    printedVal = a +var_b;
  print(printedVal) ;
  
"""


# Crea el objeto tokenizer
patokenizer = Tokenizer(dictio_regex, keyword_list)
# Tokeniza input_program
tokens = patokenizer.tokenize(lines)
# Salida esperada: el stream de tokens por linea,
# print("all found tokens", tokens, "\n")
# y la tabla de simbolos
# for key, token in tokens:
#   print('Key:', key, 'Token:', token)

patokenizer.prints()

"""Ejemplo de la estructura del stream de tokens:"""

