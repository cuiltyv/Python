# -*- coding: utf-8 -*-
"""descenso rec - base - 501.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1krqjyBNfrRrUuQEDkM2_jizf0IfPIFGG
"""

# Una clase tokens basica...
class Tokens:
  # Atributos
  tokens = []       # una lista de tokens
  pos = 0           # un indice para el token actual

  # Constructor de la clase...
  def __init__(self, lista):
    self.tokens = lista
    self.pos = 0

  # Devuelve el token actual
  def current(self):
    return self.tokens[self.pos]

  # Consume un token, avanzando al siguiente
  def avanza(self):
    self.pos += 1
    return self.tokens[self.pos]

# Guarda errores en una lista
def addError(errors, expected, token, index):
  #print(  f"ERROR index {index}: esperaba {expected}, recibio {token}"  )
  errors.append( f"ERROR en index {index}: esperaba {expected}, recibio {token}"  )



# F ->  ( E ) | const_int
def factor(tokens, errors):
  # Obten el token actual
  # token, content = tokens.current()  # <------ el tipo de token, y el valor  ej. 'const_int', 14


  # Si es un  '('

      # Avanza al siguiente token

      # Llama a expr

      # Cuando expr termine, actualiza el token actual

      # Si el token actual es ahora un ')'....

        # Avanza al siguiente

      # Si no, guarda el error

  # O si es un 'const_int'...

    # Avanza al siguiente token

  # Si no es ninguna de las anteriores...
  else:
    # guarda el error usando addError



# T' -> * F T' | epsilon
def termino_prime(tokens, errors):
  # Obten el token actual


  # Si el token actual es un '*'

    # Avanza al siguiente token

    # Llama a factor

    # Llama a termino_prime

  #Se va por epsilon



# T -> F T'
def termino(tokens, errors):
  # Llama a factor

  # Luego llama a termino_prime



# E' -> + T E' | epsilon
# <expresion_prime> ::= + <termino> <expresion_prime> | <vacio>
def expr_prime(tokens, errors):
  # Obten el token actual


  # Si el token actual es un '+'

    # Avanza al siguiente token

    # Llama a termino

    # Luego llama a expr_prime

  # Si no, nada... se asume que es <vacio>



# E -> T E'
# <expresion> ::= <termino> <expresion_prime>
def expr(tokens, errors):
  # Llama a termino
  termino(tokens, errors)
  # Luego, llama a expresion prime...
  expr_prime(tokens, errors)


#    Cada linea tiene un EOL al final, para evitar desbordar
#    El tokenizer deberia producirlo
lineas = [  [  ["const_int", 14], ["op_suma", "+"],  ["del_par_open","("], ["const_int", 14],  ["op_mul", "*"], ["const_int", 14], ["del_par_close", ")"], ["EOL",""]] ,  # ok
            [  ["const_int", 12] , ["op_suma", "+"], ["EOL", ""] ],
            [  ["const_int", 12] , ["op_suma", "+"], ["const_int", 12], ["EOL", ""] ],
            [  ["const_int", 12] , ["op_mul", "*"], ["const_int", 12], ["EOL", ""] ],
            [  ["op_mul", "*"], ["const_int", 12], ["EOL", ""] ],
            [  ["op_suma", "+"], ["const_int", 12], ["EOL", ""] ],
            [  ["del_par_open", "(" ], ["const_int", 12],  ["op_suma", "+"], ["const_int", 12], ["EOL", ""] ],
         ]

for linea in lineas:
  print("\n",linea)

  tokens = Tokens(linea)
  errors = []

  expr(tokens, errors )   #    Revisa si la linea es una expresion valida


  if tokens.pos < len(tokens.tokens)-1:       #   Si no se consumio toda la linea, hubo algun token inesperado
    addError( errors, "operador", tokens.current() , tokens.pos )

  if len(errors) == 0 :
    print("OKS\n")
  else:
    for e in errors:
      print(e)
    #print("NOPE\n", tokens.pos)

